---
title: Benefits and risks
---

import CookieBanner from "../components/cookies"

Data-driven technologies, such as artificial intelligence (AI), have the potential to bring about significant benefits for our economy and society. AI has been harnessed to help us combat the pandemic; from enabling the discovery of new vaccines and rapid virus detection methods, to powering dashboards that help clinicians to make crucial treatment decisions on the frontline. In 2020 DeepMind’s deep-learning programme AlphaFold made a huge leap forward in solving one of biology's greatest challenges, the protein folding problem - how to determine a protein's 3-dimensional shape from its amino acid sequence. This breakthrough could vastly accelerate efforts to understand the building blocks of cells and could improve and speed up drug discovery.

AI presents game changing opportunities in other sectors, too, through the potential for operating an efficient green energy grid, tackling misinformation on social media platforms, and using automated decision support systems in finance and criminal justice to minimise bias and create a fairer society for all.

However, AI systems also introduce risks that need to be managed. The autonomous, complex and scalable nature of AI systems pose risks beyond that of regular software. The autonomous nature of AI systems makes it difficult to assign accountability to individuals if harms occur; the complexity of AI systems often prevents users or affected individuals from understanding the link between a system’s output or decision and its causes, providing further challenges to assigning accountability; and the scalability of AI makes it particularly difficult to define legitimate values and governance frameworks for a system’s operation e.g. across social contexts or national jurisdictions.

In particular, machine learning systems solve problems in ways that are not directly programmed, which provides huge benefits. However, this creates risks due to the unpredictability and opacity of decisions, and challenges our existing methods for mitigating the risks of these decisions.
As these technologies are more widely adopted, there is an increasing need for a range of actors, including regulators, developers, executives, and frontline users, to check that these tools are functioning as expected and to demonstrate this to others.
However, these actors often lack the information or specialist knowledge to do this. To address this gap, effective AI assurance is required to enable these actors to make informed judgements about these systems .
