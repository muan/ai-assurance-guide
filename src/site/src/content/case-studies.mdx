---
title: Assurance case studies and examples
disableTableOfContents: true
---

import CookieBanner from "../components/cookies"
import OutboundLink from "../components/outbound-link"
import License from "../components/license"

<CookieBanner />

## Third party assurance frameworks and/or providers

| Who                                                                                                                                       | Assurance service                                                                                                                                                                                                       | Target assurance user                                                                                                                  | Description / techniques used                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Stage of development                                                        |
| :---------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------- |
| <OutboundLink href="https://www.coe.int/en/web/artificial-intelligence/cahai">**CAHAI**</OutboundLink>                                    | <OutboundLink href="https://rm.coe.int/cahai-pdg-2021-05-2768-0229-3507-v-1/1680a291a3">Impact assessment</OutboundLink>                                                                                                | Private and Public sector actors in the AI supply chain.                                                                               | <p>CAHAI (the Council of Europe’s Committee on AI and Human Rights) are producing an impact assessment for organisations developing, procuring or deploying AI to identify and mitigate the potential impacts of AI systems on human rights, democracy and the rule of law. </p><p>The impact assessment combines elements of Algorithm impact assessment with the human rights due diligence process. The impact assessment combines an analysis of potential risks of AI systems with an assessment of the potential impacts of these risks on human rights, democracy and the rule of law. The impact assessment involves stakeholder engagement and assessment of the governance and mitigation measures.</p><p>The impact assessment will support a potentially legally binding instrument to be produced by the Council of Europe, governing the use of AI.</p>                                                                                                                                                                                                                                                                                                    | Currently at the drafting stage by the CAHAI Policy Development Group (PDG) |
| <OutboundLink href="https://orcaarisk.com/">**ORCAA**</OutboundLink>                                                                      | AI risk management                                                                                                                                                                                                      | Companies procuring / deploying AI systems                                                                                             | <p>ORCAA’s algorithm audits identify and manage risks arising from the use of predictive models, focusing on ethical dimensions including fairness, bias and discrimination.</p><p>ORCAA’s algorithm audit comprises 4 key steps:<ol><li>Preparation - defining the use case and reviewing background documentation </li><li>Stakeholder discussions - to elicit the concerns of internal and external stakeholders.</li><li>The ethical matrix - mapping stakeholders and their concerns onto a grid to prioritise the most pressing concerns.</li><li>Planning remediation steps - coming up with ways to investigate, validate and address the priority concerns identified. Reviewing companies, preparation, validation and testing practices.</li></ol></p>                                                                                                                                                                                                                                                                                                                                                                                                        | In use                                                                      |
| <OutboundLink href="https://www.getparity.ai/">**Parity**</OutboundLink>                                                                  | Compliance and risk assessment                                                                                                                                                                                          | Organisations developing or deploying AI systems.                                                                                      | <p>Parity provides a 3rd party model risk assessment platform for considering compliance, risk and impact of an AI system.</p><p>Parity offers a range of tools beginning with an impact assessment to bias mitigation tools, continuous algorithm monitoring and explainability platforms.</p><p>The parity enables organisations to connect up their model cards to their platform to provide model documentation for analysis. The platform first asks the organisation to build an impact assessment which is used to identify risk zones, algorithm assessment methods and methods to mitigate harmful model qualities like disparate impact.</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | In use                                                                      |
| <OutboundLink href="https://arthur.ai/product">**Arthur**</OutboundLink>                                                                  | AI monitoring                                                                                                                                                                                                           | Developers, in-house data scientists, compliance/risk officers.                                                                        | <p>Arthur is a model monitoring platform that gives organisations visibility over the performance of their AI models. </p><p>Arthur provides performance monitoring, algorithmic bias detection, and explainability as a service, so data science teams can detect, diagnose, and fix issues in production.</p><p>Arthur are partnered with the MLOPS and management platform, Algorithmia, which manages all stages of ML lifecycle within existing organisational processes. </p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | In use                                                                      |
| <OutboundLink href="https://wandb.ai/site">**Weights and Biases**</OutboundLink>                                                          | Developer first ML tools platform                                                                                                                                                                                       | ML practitioners                                                                                                                       | <p>Weights and Biases assurance strategy aims to build a similar stack of services for ML practitioners that exist in the software world for ML developers to deploy code well e.g. GitLab and GitHub.</p><p>The weights and biases stack provides visualisation tools to assist the development of machine learning research to help companies to turn their research into deployed models. Weights and biases helps development teams to track their models, visualise performance and automote training and model improvement.</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | In use                                                                      |
| **ICO**                                                                                                                                   | <OutboundLink href="https://ico.org.uk/media/2617219/guidance-on-the-ai-auditing-framework-draft-for-consultation.pdf">Guidance on AI and Data Protection</OutboundLink>                                                | Developers and Assurance practitioners                                                                                                 | The ICO is currently developing an <OutboundLink href="https://ico.org.uk/about-the-ico/news-and-events/ai-auditing-framework/">AI Auditing Framework</OutboundLink> (AIAF), which has three distinct outputs. The first is a set of tools and procedures for their assurance and investigation teams to use when assessing the compliance of organisations using AI. The second is detailed guidance on AI and data protection for organisations. The third is an AI and data protection toolkit designed to provide further practical support to organisations auditing the compliance of their own AI systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | In use                                                                      |
| **ICO**                                                                                                                                   | <OutboundLink href="https://ico.org.uk/about-the-ico/news-and-events/blog-new-toolkit-launched-to-help-organisations-using-ai/">AI and data protection toolkit</OutboundLink>                                           | Organisations using AI                                                                                                                 | AI and data protection toolkit designed to provide further practical support to organisations auditing the compliance of their own AI systems. The toolkit is currently in beta phase with a final version expected by the end of 2021.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Beta phase                                                                  |
| **ICO and Alan Turing Institute**                                                                                                         | <OutboundLink href="https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/explaining-decisions-made-with-ai/">Explaining decisions made with AI guidance</OutboundLink>                           | Organisations using AI                                                                                                                 | The ICO has also published guidance with the Alan Turing Institute, Explaining Decisions Made With AI which gives practical advice to help organisations to explain the processes, services and decisions delivered or assisted by AI, to the individuals affected by them.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | In use                                                                      |
| **MHRA**                                                                                                                                  | <OutboundLink href="https://www.gov.uk/government/publications/software-and-ai-as-a-medical-device-change-programme">Software and AI as a Medical Device Change Programme</OutboundLink>                                | Public or private organisations using AI as part of a medical device                                                                   | The Medicines and Healthcare Products Regulatory Agency (MHRA) has developed an extensive work programme to update regulations applying to software and artificial intelligence as a medical device. This work reflects the increasingly prominent role of AI within health systems and will cover the regulation of software and artificial intelligence as a medical device across the lifecycle. The reforms will offer clear assurance guidance on how to interpret regulatory requirements and how to demonstrate conformity.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | In development                                                              |
| **ForHumanity**                                                                                                                           | <OutboundLink href="https://forhumanity.center/independent-audit-of-ai-systems/">Independent Audit for AI systems (IAAIS)</OutboundLink>                                                                                | Audit rules are applicable to both public and private entities but the comprehensive process is targeted at publicly traded companies. | <p>ForHumanity’s approach to Independent Audit for AI Systems provides a framework to audit AI Safety in the categories of Bias, Privacy, Ethics, Trust and Cybersecurity. </p><p>The aim of the audit is to build an ‘infrastructure of trust’. To create this infrastructure of trust, ForHumanity have set out five criteria to ensure that rules are auditable:<ol><li>Binary - compliant/non-compliant</li><li>Measurable/unambiguous</li><li>Iterated and open source</li><li>Consensus driven</li><li>Implementable</li></ol></p><p>ForHumanity emphasise independence and auditability as two key components in building trust:<ul><li>Auditability ensures that organisations can demonstrate and communicate compliance.</li><li>Independence ensures that an audit practitioner has to attest to compliance, ensuring accountability (because they have something to lose if they are wrong)</li></ul></p>                                                                                                                                                                                                                                                    | In use                                                                      |
| **ForHumanity**                                                                                                                           | <OutboundLink href="https://forhumanity.center/uk-gdpr/">UKGDPR Certification framework</OutboundLink>                                                                                                                  | Data processors, controllers and joint controllers                                                                                     | <p>ForHumanity’s UKGDPR Certification framework responded to the ICO’s call to develop a certification scheme for UKGDPR compliance. It is the first of its kind.</p><p>The certification framework can be applied to all AI and data driven systems where personal data is being processed. </p><p>A critical aspect of ForHumanity’s certification scheme is robust governance, oversight and accountability at the Officer and Board of Directors level to ensure trust. Simply requiring an organisation to comply with certification criteria without defining specific accountability in governance structures creates ambiguity regarding delegation of authority and/or segregation of duties. </p><p>The certification scheme requires the establishment of an Algorithmic Risk Committee, Ethics Committee and Children’s Data Oversight committees to ensure conflict free, objective decision-making.</p><p>ForHumanity audit criteria is designed to ensure human agency is embedded in all algorithmic systems, Artificial Intelligence and Autonomous systems.</p>                                                                                        | Submitted to the ICO                                                        |
| **SHERPA Project**                                                                                                                        | <OutboundLink href="https://www.project-sherpa.eu/ai-impact-assessment/">Baseline model for AI impact assessment </OutboundLink>                                                                                        | EU-based developers or users of AI systems                                                                                             | <p>The Sherpa project’s aim is to develop baseline recommendations for the Eurpoean commission for developing an AI impact assessment. </p><p>The SHERPA project has put forward recommendations for guidance in four key areas:<ol><li>Processes for conducting and evaluating AI impact assessments</li><li>Measures and metrics for AI impact assessment</li><li>Determination of risk level for a technology or application</li><li>Issues to be included in an AI impact assessment</li></ol></p><p>They also recommend that AI impact assessment should build on numerous existing impact assessments e.g. data protection, human rights, ethics, environmental and socio-economic impact assessment. </p><p>To accompany the recommendations, SHERPA has submitted accounts of the ethical and human rights issues of AI as part of its case study research. They have also built likely future scenarios to inform design. </p>                                                                                                                                                                                                                                  | Recommendations submitted                                                   |
| **PwC**                                                                                                                                   | <OutboundLink href="https://www.pwc.com/gx/en/issues/data-and-analytics/artificial-intelligence/what-is-responsible-ai.html">Responsible AI Framework and Toolkit</OutboundLink>                                        | Industry facing                                                                                                                        | <p>The Toolkit contains a set of customisable frameworks, tools, processes, leading practices and practice aids to help organisations to define requirements for responsible AI governance.</p><p>The toolkit contains an initial free responsible AI diagnostic for organisations to evaluate their performance relative to industry peers, generating a score with actions to consider. </p><p>Ther toolkit addressed the ‘three dimensions of responsible AI’<ol><li>Strategy<ol><li>Data and AI ethics</li><li>Policy and regulation</li></ol></li><li>Performance and security<ul><li>Bias and fairness</li><li>Interpretability and explainability</li><li>Privacy</li><li>Security</li><li>Robustness</li><li>Safety</li></ul></li><li>Control<ul><li>Governance</li><li>Compliance</li><li>Risk management</li></ul></li></ol></p><p>The toolkit is structured as an end to end process covering the AI lifecycle and is designed around building trust </p>                                                                                                                                                                                                     | In use                                                                      |
| <OutboundLink href="https://www.holisticai.com/">**Holistic AI**</OutboundLink>                                                           | AI risk management services                                                                                                                                                                                             | Industry facing                                                                                                                        | <p>Holistic AI is a regtech startup focused on governance and internal auditing of AI systems in accordance with regulatory and ethical standards. Holistic AI is a platform service that provides automated risk assessment and assurance for companies deploying AI systems.</p><p>Holistic AI’s risk assessment platform offers a range of tools including bias audit, performance testing and continuous monitoring, to provide assurance across verticals of bias, privacy, explainability and robustness. </p><p>Holistic’s approach combines expert auditing for regulatory and ethical requirements with real time AI monitoring applications for businesses. </p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | In use                                                                      |
| **Government of Canada**                                                                                                                  | <OutboundLink href="https://www.canada.ca/en/government/system/digital-government/digital-government-innovations/responsible-use-ai/algorithmic-impact-assessment.html">Algorithm impact assessment tool</OutboundLink> | Government departments and agencies                                                                                                    | <p>The Algorithm Impact Assessment (AIA) is a mandatory questionnaire that determines the impact level of an automated decision-system. </p><p>The tool is composed of 48 risk and 33 mitigation questions. Assessment scores are based on factors including the system design, algorithm, decision type, impact and data. </p><p>The assessment is organised according to the Canadian Government’s policy, ethical, and administrative law considerations of automated decision making risk areas. It is designed to help departments and agencies better understand and manage AI risks. </p><p>The tool outlines 6 key risk areas:<ol><li>Project</li><li>System</li><li>Algorithm</li><li>Decision</li><li>Impact</li><li>Data</li></ol></p><p>The tool outlines two further mitigation areas:<ol><li>Consultation</li><li>De-risking and mitigation measures</li></ol></p><p>The value of each question in the impact assessment is weighted based on the raw impact and mitigation.</p><p>Completion of the AIA indicates an impact level for the system in question. Impact levels range from little impact (level 1) to very high impact (level 4)</p>          | In use                                                                      |
| **Responsible Artificial Intelligence Institute**                                                                                         | <OutboundLink href="https://www.responsible.ai/how-we-help#:~:text=The%20First%20Independent%2C%20Accredited%20Certification%20Program%20for%20Responsible%20AI">RAI Certification</OutboundLink>                                                                                                                         | Practitioners creating responsible AI                                                                                                  | <p>RAI certification is used by organisations to demonstrate that an AI system has been designed, built, and deployed in line with the five OECD Principles on Artificial Intelligence:<ol><li>Explainability</li><li>Fairness</li><li>Accountability</li><li>Robustness</li><li>Data Quality</li></ol></p><p>The five OECD categories of responsible AI are used as parameters for the different credit elements within the RAI Certification rating system.</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Beta phase                                                                  |
| <OutboundLink href="https://www.fiddler.ai/">**Fiddler**</OutboundLink>                                                                   | Machine learning monitoring and explainable AI                                                                                                                                                                          | Businesses developing / deploying AI                                                                                                   | <p>Fiddlers platform offers continuous ML monitoring and automated AI explainability for an organisation to monitor, observe, and analyse their AI models. </p><p>Fiddlers platform send real time alerts to users when models behave unexpectedly, providing actionable insights. The platform enables users to pinpoint data drift, including contributing features to alert users when they need to retarian their models and highlights model inference violations when they happen for proactive mitigation. </p><p>Fiddler’s methodology is based on a feedback loop beginning with offline steps:<ol><li>Train - Logging training and test datasets to check bias and feature quality</li><li>Validate - Explain the model, discover low performance and create models dashboards and reports</li></ol></p><p>This is followed by three online steps:<ol><li>Deploy - Recoring model traffic and metadata, comparing new and existing models.</li><li>Monitor - Observing performance drift, outliers, bias and errors on conditions and slices.</li><li>Analyse - Analyse performance issues, identify root causes, generate adversarial examples.</li></ol></p> | In use                                                                      |
| <OutboundLink href="https://babl.ai/">**babl**</OutboundLink>                                                                             | Consulting and AI auditing                                                                                                                                                                                              | Businesses developing / deploying AI                                                                                                   | <p>babl have developed a holistic framework for auditing and evaluating algorithms based on fairness, effectiveness and transparency, with the goal of helping organisations to build trust. </p><p>babl offer two services:<ul><li>Consulting - which helps organisations to develop and deploy AI and machine learning models in a may that mitigates reputational and compliance risk, and better align with organisational and public values. </li><li>Algorithm auditing - uses babl’s ethical algorithm auditing framework to ensure that algorithms being deployed don’t compromise consumer or employee trust.</li></ul></p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | In use                                                                      |
| <OutboundLink href="https://ethicsgrade.io/">**EthicsGrade**</OutboundLink>                                                               | Environmental, Social and Governance (ESG) ratings                                                                                                                                                                      | Businesses using AI                                                                                                                    | <p>EthicsGrade, grade and score companies on their AI governance. They produce a publicly available scorecard based on their assessments which are based on ESG responsibilities.</p><p>EthicsGrade use a three step process to generate a scorecard:<ol><li>Collect - Information is collected from publicly available resources, information submitted by the company through a survey, and through internal assessment.</li><li>Analyse - Information is analysed through 6 research dimensions. Structure, public policy, technical barriers to trust, ethical risk, data privacy, sustainability.</li><li>Grade - Three types of scorecard are updated based on available information: Baseline scorecard, InsideView Scorecard and Benchmark Scorecard.</li></ol></p>                                                                                                                                                                                                                                                                                                                                                                                              | In use                                                                      |
| <OutboundLink href="https://ethicalai.ai/">**EthicalAI**</OutboundLink>                                                                   | Ethical AI frameworks (inc. human rights) and application toolkits                                                                                                                                                      | Strategic advice to Boards, CEO’s, exec teams and leaders in the private, public and community sectors.                                | <p>EthicalAi’s advisory services include: Helping organisations to design and develop ethics frameworks and providing AI awareness and education to leadership teams.</p><p><ul><li>Ethical AI provide education on the principles and frameworks involved in ethical AI that meet government or international guidelines. </li><li>Provide design and development workshops to support leadership teams to build ethics frameworks specific to their organisation, industry sector, clients and objectives.</li><li>Provide design and development toolkits to help organisations to implement ethical AI principles contained in their ethics frameworks and to assess the effectiveness and operationalisation of ethics principles.</li></ul></p>                                                                                                                                                                                                                                                                                                                                                                                                                    | In use                                                                      |
| **AI Now Institute**                                                                                                                      | <OutboundLink href="https://ainowinstitute.org/aiareport2018.pdf">Algorithm Impact Assessment</OutboundLink> (based on environmental impact assessment)                                                                 | Public Agencies                                                                                                                        | <p>AI Now Institute’s Impact Assessment report provides a practical framework, similar to an environmental impact assessment, for agencies to bring oversight to automated decision systems.</p><p>Their framework helps affected communities and stakeholders assess the use of AI and algorithmic decision-making in public agencies and determine where – or if – their use is acceptable.</p><p>Key elements of the Public Agency Algorithmic Impact Assessment:<ol><li>Agencies should conduct a self assessment of automated decision systems evaluating potential impacts on fairness, justice, bias, or other concerns across affected communities</li><li>Agencies should develop meaningful external researcher review processes to measure and track impact over time.</li><li>Agencies should provide notice to the public disclosing their definition of ‘automated decision system’, existing and proposed systems, and any related self-assessments before the system has been acquired.</li><li>Agencies should solicit public comments to clarify concerns and answer questions.</li></ol></p>                                                          | In use                                                                      |
| <OutboundLink href="https://solutionscenter.nethope.org/artificial-intelligence-ethics-for-nonprofits-toolkit">**Nethope**</OutboundLink> | AI Ethics and AI Suitability toolkits                                                                                                                                                                                   | Non profits                                                                                                                            | <p>The goal of the AI suitability toolkit is to increase NGO’s internal expertise and capacity to evaluate, develop, procure and use AI and ML in their work so that they can make informed decisions, improve their work and anticipate issues that might arise. </p><p>The toolkit includes an ‘AI Workshop for Nonprofits’ and an AI suitability framework. The workshop and materials provide participants with an overview of AI/ML and how to evaluate their suitability for projects. </p><p>The goal of the ethics toolkit is to build capacity in the non-profit sector to design, deploy and use AI responsibly and ethically. </p><p>The first installment of the Ethics Toolkit provides the materials needed to host an AI ethics Workshop focused on the ethical considerations related to fairness. In the workshops, participants learn about fundamentals of AI ethics and practice applying ethical considerations related to fairness in the context of humanitarian and international development use cases. </p>                                                                                                                                    | In use                                                                      |
| <OutboundLink href="https://www.saidot.ai/">**Saidot**</OutboundLink>                                                                     | AI governance and compliance - Integrating AI responsibility in Corporate Social Responsibility (CSR) Disclosures                                                                                                       | Governments and private companies                                                                                                      | <p>Saidot provides a platform for responsible AI and transparency. The platform enables organisations to build systematic AI governance workflows and documentation methodology in line with ethical and regulatory requirements. </p><p>The saidot platform enables organisations to register their algorithmic systems to give a systematic view of their AI portfolio - this provides the basis for systematic governance and transparent communication.</p><p>Saidot’s transparency tools help organisations to communicate about their AI system to customers and other stakeholders. Saidot provide a modular metadata model to standardise transparency and adapt to organisations sector specific requirements. </p><p>When information is created in the platform, Saidot help to adapt it to different audiences and channels of communication. </p>                                                                                                                                                                                                                                                                                                           | In use                                                                      |
| <OutboundLink href="https://www.imandra.ai/">**Imandra**</OutboundLink>                                                                   | Formal verification                                                                                                                                                                                                     | Financial markets                                                                                                                      | <p>Imandra provides what they call ‘reasoning as a service’. Imandra is a general-purpose automated reasoning engine that leverages formal verification methods to offer a suite of tools that can be used to provide insights into algorithm performance and guarantees for a wide range of algorithms.</p><p>Imandra’s automated reasoning and analysis engine aims to mathematically verify algorithm properties and systematically explore and elucidate possible algorithm behaviors. </p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | In use                                                                      |
| <OutboundLink href="https://etiq.ai/">**Etiq AI**</OutboundLink>                                                                          | Algorithmic bias, accuracy, explainability                                                                                                                                                                              | Businesses using AI                                                                                                                    | <p>Etiq AI is a customisable software platform for data scientists, risk and business managers in the insurance, lending, and technology industries.</p><p>The platform offers testing, monitoring, optimisation and explainability solutions to allow users to identify and mitigate the unintended bias in machine learning algorithms.</p>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | In use                                                                      |
| **Deloitte**                                                                                                                              | <OutboundLink href="https://www2.deloitte.com/us/en/pages/deloitte-analytics/solutions/ethics-of-ai-framework.html">Trustworthy AI framework</OutboundLink>                                                             | Industry facing                                                                                                                        | <p>Deloitte’s trustworthy AI solutions include a six pillar Trustworthy AI framework which aims to guide organizations on how to apply AI responsibly and ethically within their businesses. The six pillars are:<ol><li>Fair and impartial</li><li>Robust and reliable</li><li>Privacy</li><li>Safe ans secure</li><li>Responsible and accountable</li><li>Transparent and explainable</li></ol></p><p>Deloitte use a three stage AI risk management framework focused on:<ol><li>Identifying and assessing risks</li><li>Ensuring existing activities meet required standards</li><li>Monitoring and tracking key risk indicators</li></ol></p><p>Deloitte have also developed AlgoInsight: an AI explainability toolkit to facilitate assessments of bias and performance.</p>                                                                                                                                                                                                                                                                                                                                                                                        | In use                                                                      |

<License />
