---
title: The role of technical standards in AI assurance
---

import CookieBanner from "../components/cookies"
import Collapse from "../components/collapse"

<CookieBanner />

Technical standards are essentially agreed ways of doing things - making a product, managing a process, delivering a service or supplying materials. The point of a standard is to provide a reliable, shared basis for people to share the same expectations about a product, process system or service.

Technical standardisation can help to make subject matter more objectively measurable, which can help with the adoption of more scalable assurance methods.

- Foundational standards help build common understanding around definitions and terminology, making them less ambiguous (more explicit). Foundational (technical) standards enable dialogue between assurance users and providers, facilitating greater trust throughout the ecosystem by providing common grounds for assurance users to assess and challenge assurance processes. For example, ISO/IEC DIS 22989 Information technology — Artificial intelligence — Artificial intelligence concepts and terminology
- Process standards help to universalise best practice in organisational management, governance and internal control as a way to achieve good outcomes via organisation processes. Technical process standards institutionalise trust building practices such as risk and quality management. Some process standards are ‘certifiable’, meaning that organisations can be independently assessed as meeting ‘good practice’ and can receive a certification against that technical standard. For example, ISO/IEC CD 42001 - Information Technology - Artificial Intelligence - Management System.
- Measurement standards provide common definitions for quantitative measurement of particular aspects of performance. This helps to make the subject matter more explicit and mutually understood, enabling more scalable assurance methods. This can also be beneficial for assurance, even if there are multiple (mutually incompatible) measurement standards, or where it is not feasible, helping to improve the transparency and accessibility of assurance processes even where technical standards for performance can’t be agreed. For example, ISO/IEC DTS4213 Assessment of Machine Learning Performance describes best practices for comparing the performance of ML models, and specifies classification performance metrics and reporting requirements.
- Performance standards set specific thresholds for acceptability.

Technical standards at all of these levels can help to deliver mutually understood and scalable AI Assurance.

Where independent assurance is valuable, agreed technical standards are needed to enable assurance users to trust the evidence and conclusions presented by assurance service providers. Standards enable very clear communication of a systems trustworthiness i.e. "this standard has been met/hasn't been met". Existing AI 'auditing' efforts are done without commonly accepted standards - this is more like 'advisory' services which are useful, but rely heavily on the judgement of the ‘auditor’.

Without agreed technical standards or common understanding around acceptable performance or levels of risk, a disconnect between the values and opinions of different actors can prevent assurance from building trust. For example, an assurance user might disagree with the views of an assurance provider about the appropriate scope of an impact assessment, or how to measure the level of accuracy of a system.

<Collapse label="Case study: Setting conventions for reasonable expectations of fairness">

We think in general, setting standards for fairness is context specific. However, the 4/5ths rule provides an example of a legal convention (used in the United States) to make judgements of fairness more explicit and transferable without relying on consensus or agreement.

The 4/5ths rule prescribes that the selection rate for any group (classified by a protected characteristic) that is less than 4/5th’s that of the group with the highest rate constitutes evidence of adverse impact that has discriminatory effects on the protected group. Because what is deemed fair in any given circumstance is contextual and subjective, a standardised impact ratio cannot determine whether something is actually fair.

However, the 4/5ths rule is not claiming to do this, rather it sets a standard that regulators deem to be reasonable, rather than objectively fair. By standardising a reasonable level of fairness, the 4/5ths rule has the benefit of making assessments of fairness efficient and scalable. Without such a convention, an organisation making a decision is accountable for choosing a definition of fairness and deciding on the outcomes they deem appropriate. The 4/5ths rule removes ambiguity around assessments of fairness by creating an explicit benchmark, even though this might not always accurately correspond to the ‘fairest’ impact ratio or distribution of outcomes.

In some contexts, we might be able to develop conventions that become common practice (eg 4/5ths rule), but we shouldn’t pretend that we have actually created an objective criteria that is universal or in the case of the 4/5ths rule, one that actually designates what is considered to be fair in a given situation.

By creating a convention that can be followed, the 4/5ths rule shifts accountability for decisions about fairness onto the regulator. While this improves the scalability and efficiency of assessing fairness for organisations, it risks absolving organisations using AI systems of their responsibility to mitigate bias and might lead to poorer outcomes overall. This is an example of a tradeoff that will have to be balanced when developing standards for AI systems.

</Collapse>
